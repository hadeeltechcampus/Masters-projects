{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kI9i_q1Bo7Wb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import pycocotools\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_snippets import Report\n",
        "import time\n",
        "import torchvision.transforms.functional as F\n",
        "import torchvision.transforms.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install torch_snippets"
      ],
      "metadata": {
        "id": "ZEmwzN4MdPrp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get annotation\n",
        "def get_xmlfilenames(dire):\n",
        "  xml_filenames=[]\n",
        "  for filename in os.listdir(dire):\n",
        "    if filename.endswith(\".xml\"):\n",
        "      xml_filenames.append(filename)\n",
        "  return sorted(xml_filenames)"
      ],
      "metadata": {
        "id": "elydxlsXpJD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_str2num={'with_mask': 1, 'without_mask': 2, 'mask_weared_incorrect': 3}\n",
        "class_num2str = {v: k for k, v in class_str2num.items()}\n",
        "\n",
        "def parse_xml(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    bboxes = []\n",
        "    labels = []\n",
        "    filename = root.find('filename').text\n",
        "    for boxes in root.iter('object'):\n",
        "        ymin, xmin, ymax, xmax = None, None, None, None\n",
        "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
        "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
        "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
        "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
        "        box = [xmin, ymin, xmax, ymax]\n",
        "        bboxes.append(box)\n",
        "        labels.append(int(class_str2num[boxes.find(\"name\").text]))\n",
        "    return filename, bboxes, labels"
      ],
      "metadata": {
        "id": "DcSS0_5VpKtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get xml files\n",
        "def xml_to_dict(xml_path):\n",
        "    # Decode the .xml file\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    # Return the image size, object label and bounding box\n",
        "    # coordinates together with the filename as a dict.\n",
        "    return {\"filename\": xml_path,\n",
        "            \"image_width\": int(root.find(\"./size/width\").text),\n",
        "            \"image_height\": int(root.find(\"./size/height\").text),\n",
        "            \"image_channels\": int(root.find(\"./size/depth\").text),\n",
        "            \"label\": root.find(\"./object/name\").text,\n",
        "            \"x1\": int(root.find(\"./object/bndbox/xmin\").text),\n",
        "            \"y1\": int(root.find(\"./object/bndbox/ymin\").text),\n",
        "            \"x2\": int(root.find(\"./object/bndbox/xmax\").text),\n",
        "            \"y2\": int(root.find(\"./object/bndbox/ymax\").text)}"
      ],
      "metadata": {
        "id": "7uX6YuEwpWdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert human readable str label to int.\n",
        "label_dict = {'with_mask': 1, 'without_mask': 2, 'mask_weared_incorrect': 3}\n",
        "# Convert label int to human readable str.\n",
        "reverse_label_dict = {1:'with_mask', 2:'without_mask', 3:'mask_weared_incorrect'}\n",
        "\n",
        "\n",
        "class MaskedFaceDataset(torch.utils.data.Dataset):\n",
        "     def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        self.imgs=sorted(glob.glob(os.path.join(root,\"*.png\")))\n",
        "        self.antns=get_xmlfilenames(root)\n",
        "\n",
        "     def __getitem__(self, idx):\n",
        "        # load annotation\n",
        "        filename, boxes, labels = parse_xml(os.path.join(self.root,self.antns[idx]))\n",
        "        # load image\n",
        "        img_path = os.path.join(self.root, filename)\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        num_objs = boxes.shape[0]\n",
        "        # classes\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        image_id = int(torch.tensor([idx]))\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        w,h = img.size\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "        keep = (boxes[:, 3]>boxes[:, 1]) & (boxes[:, 2]>boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        labels = labels[keep]\n",
        "        area = area[keep]\n",
        "        iscrowd = iscrowd[keep]\n",
        "\n",
        "        target = {}\n",
        "        target['boxes'] = boxes\n",
        "        target['labels'] = labels\n",
        "        target['image_id'] = image_id\n",
        "        target['area'] = area\n",
        "        target['iscrowd'] = iscrowd\n",
        "        if self.transforms is not None:\n",
        "             img,target = self.transforms(img,target)\n",
        "        return  img,target\n",
        "\n",
        "     def __len__(self):\n",
        "        return len(self.antns)"
      ],
      "metadata": {
        "id": "P820qe-prV7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform helper\n",
        "class Compose:\n",
        "\n",
        "    def __init__(self, transforms = []):\n",
        "        self.transforms = transforms\n",
        "    # __call__ sequentially performs the image transformations on the input image, and returns the augmented image.\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "UJbh2w-DrV1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transforms class\n",
        "class ToTensor(torch.nn.Module):\n",
        "\n",
        "    def forward(self, image, target = None):\n",
        "        image = F.pil_to_tensor(image)\n",
        "        image = F.convert_image_dtype(image)\n",
        "        return image, target\n",
        "class RandomHorizontalFlip(T.RandomHorizontalFlip):\n",
        "\n",
        "    def forward(self, image, target = None):\n",
        "        if torch.rand(1) < self.p:\n",
        "            image = F.hflip(image)\n",
        "            if target is not None:\n",
        "                width, _ = F.get_image_size(image)\n",
        "                target[\"boxes\"][:, [0, 2]] = width - \\\n",
        "                                     target[\"boxes\"][:, [2, 0]]\n",
        "        return image, target"
      ],
      "metadata": {
        "id": "oygT_sl5rVz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # ToTensor is applied to all images.\n",
        "    transforms.append(ToTensor())\n",
        "    # The following transforms are applied only to the train set.\n",
        "    if train == True:\n",
        "        transforms.append(RandomHorizontalFlip(0.5))\n",
        "        # Other transforms can be added here later on.\n",
        "    return Compose(transforms)"
      ],
      "metadata": {
        "id": "sQjB71TyrVxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = MaskedFaceDataset('/content/drive/MyDrive/Colab Notebooks/MaskedFace/train', transforms =get_transform(train=True) )\n",
        "test_ds = MaskedFaceDataset('/content/drive/MyDrive/Colab Notebooks/MaskedFace/val', transforms = get_transform(train=False))\n",
        "val_ds = MaskedFaceDataset('/content/drive/MyDrive/Colab Notebooks/MaskedFace/val', transforms = get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "train_indices = torch.randperm(len(train_dataset)).tolist()\n",
        "test_indices = torch.randperm(len(test_dataset)).tolist()\n",
        "\n",
        "# Please feel free to use more samples if you have enough resources\n",
        "n = 500\n",
        "#train_ds = torch.utils.data.Subset(train_dataset, train_indices[:n])\n",
        "#test_ds = torch.utils.data.Subset(test_dataset, test_indices[:n])\n",
        "#val_ds=torch.utils.data.Subset(val_ds, test_indices[:n])"
      ],
      "metadata": {
        "id": "ap0H4WoxrVqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collate image-target pairs into a tuple.\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "# Create the DataLoaders from the Datasets.\n",
        "train_dl = torch.utils.data.DataLoader(train_ds,\n",
        "                                 batch_size = 4,\n",
        "                                 shuffle = True,\n",
        "                        collate_fn = collate_fn)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds,\n",
        "                             batch_size = 4,\n",
        "                            shuffle = False,\n",
        "                    collate_fn = collate_fn)\n",
        "test_dl = torch.utils.data.DataLoader(test_ds,\n",
        "                               batch_size = 4,\n",
        "                              shuffle = False,\n",
        "                      collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "KtOBSOtvrVoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "def get_object_detection_model(num_classes = 4,\n",
        "                               feature_extraction = True):\n",
        "\n",
        "    # Load the pretrained faster r-cnn model.\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained = True)\n",
        "    # If True, the pre-trained weights will be frozen.\n",
        "    if feature_extraction == True:\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    # tailored for num_classes.\n",
        "    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats,\n",
        "                                                   num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "LgL0-otYrVho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trrain and validate using batches\n",
        "def unbatch(batch, device):\n",
        "\n",
        "    X, y = batch\n",
        "    X = [x.to(device) for x in X]\n",
        "    y = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in y]\n",
        "\n",
        "    return X, y\n",
        "def train_batch(batch, model, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, losses\n",
        "@torch.no_grad()\n",
        "def validate_batch(batch, model, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    X, y = unbatch(batch, device = device)\n",
        "    optimizer.zero_grad()\n",
        "    losses = model(X, y)\n",
        "    loss = sum(loss for loss in losses.values())\n",
        "    return loss, losses"
      ],
      "metadata": {
        "id": "0WxVUiPzte1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train model function which save data in train log report\n",
        "def train_fasterrcnn(model,\n",
        "                 optimizer,\n",
        "                  n_epochs,\n",
        "              train_loader,\n",
        "        test_loader ,\n",
        "                log = None,\n",
        "               keys = None,\n",
        "            device = \"cpu\"):\n",
        "    if log is None:\n",
        "        log = Report(n_epochs)\n",
        "    if keys is None:\n",
        "        # FasterRCNN loss names.\n",
        "        keys = [\"loss_classifier\",\n",
        "                   \"loss_box_reg\",\n",
        "                \"loss_objectness\",\n",
        "               \"loss_rpn_box_reg\"]\n",
        "    model.to(device)\n",
        "    for epoch in range(n_epochs):\n",
        "        N = len(train_loader)\n",
        "        for ix, batch in enumerate(train_loader):\n",
        "            loss, losses = train_batch(batch, model,\n",
        "                                  optimizer, device)\n",
        "            # Record the current train loss.\n",
        "            pos = epoch + (ix + 1) / N\n",
        "            log.record(pos = pos, trn_loss = loss.item(),\n",
        "                       end = \"\\r\")\n",
        "        if test_loader is not None:\n",
        "            N = len(test_loader)\n",
        "            for ix, batch in enumerate(test_loader):\n",
        "                loss, losses = validate_batch(batch, model,\n",
        "                                         optimizer, device)\n",
        "\n",
        "                # Record the current validation loss.\n",
        "                pos = epoch + (ix + 1) / N\n",
        "                log.record(pos = pos, val_loss = loss.item(),\n",
        "                           end = \"\\r\")\n",
        "    log.report_avgs(epoch + 1)\n",
        "    return log"
      ],
      "metadata": {
        "id": "t1sAHGEUtzDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the faster rcnn model with 3 classes  and  background.\n",
        "\n",
        "model = get_object_detection_model(num_classes = 4,\n",
        "                        feature_extraction = False)\n",
        "# Use the stochastic gradient descent optimizer.\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params,lr = 0.001, momentum = 0.9, weight_decay = 0.0005)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "# Train the model over 1 epoch.\n",
        "log = train_fasterrcnn(model = model,optimizer = optimizer,  n_epochs = 1, train_loader = train_dl, test_loader = test_dl,  log = None, keys = None, device = device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rZNhCnBty_4",
        "outputId": "d65fac02-d58f-441a-e916-27b9ea4293b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1.000  val_loss: 0.440  trn_loss: 0.502  (122.83s - 0.00s remaining)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs=10\n",
        "for epoch in range(n_epochs):\n",
        "      _n = len(train_dl)\n",
        "      for ix, inputs in enumerate(train_dl):\n",
        "          loss, losses = train_batch(inputs, model, optimizer,device)\n",
        "          loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n",
        "              [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n",
        "          pos = (epoch + (ix+1)/_n)\n",
        "          log.record(pos, trn_loss=loss.item(), trn_loc_loss=loc_loss.item(),\n",
        "                    trn_regr_loss=regr_loss.item(), trn_objectness_loss=loss_objectness.item(),\n",
        "                    trn_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n",
        "\n",
        "      _n = len(test_dl)\n",
        "      for ix,inputs in enumerate(test_dl):\n",
        "          loss, losses = validate_batch(inputs, model,optimizer,device)\n",
        "          loc_loss, regr_loss, loss_objectness, loss_rpn_box_reg = \\\n",
        "            [losses[k] for k in ['loss_classifier','loss_box_reg','loss_objectness','loss_rpn_box_reg']]\n",
        "          pos = (epoch + (ix+1)/_n)\n",
        "          log.record(pos, val_loss=loss.item(), val_loc_loss=loc_loss.item(),\n",
        "                    val_regr_loss=regr_loss.item(), val_objectness_loss=loss_objectness.item(),\n",
        "                    val_rpn_box_reg_loss=loss_rpn_box_reg.item(), end='\\r')\n",
        "      if (epoch+1)%(n_epochs//10)==0: log.report_avgs(epoch+1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XOCniA3gtjj",
        "outputId": "5b6a8688-e06c-4344-977b-4dd0ce913889"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1.000  val_rpn_box_reg_loss: 0.009  trn_loc_loss: 0.112  val_objectness_loss: 0.010  trn_regr_loss: 0.201  trn_objectness_loss: 0.008  trn_rpn_box_reg_loss: 0.010  val_regr_loss: 0.220  val_loc_loss: 0.129  trn_loss: 0.417  val_loss: 0.404  (239.57s - 0.00s remaining)\n",
            "EPOCH: 2.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.095  val_objectness_loss: 0.010  trn_regr_loss: 0.180  trn_objectness_loss: 0.006  trn_rpn_box_reg_loss: 0.009  val_regr_loss: 0.194  val_loc_loss: 0.119  trn_loss: 0.290  val_loss: 0.321  (350.89s - -175.45s remaining)\n",
            "EPOCH: 3.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.088  val_objectness_loss: 0.008  trn_regr_loss: 0.168  trn_objectness_loss: 0.005  trn_rpn_box_reg_loss: 0.008  val_regr_loss: 0.194  val_loc_loss: 0.116  trn_loss: 0.269  val_loss: 0.326  (461.18s - -307.45s remaining)\n",
            "EPOCH: 4.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.082  val_objectness_loss: 0.009  trn_regr_loss: 0.160  trn_objectness_loss: 0.004  trn_rpn_box_reg_loss: 0.007  val_regr_loss: 0.196  val_loc_loss: 0.115  trn_loss: 0.253  val_loss: 0.327  (572.06s - -429.04s remaining)\n",
            "EPOCH: 5.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.078  val_objectness_loss: 0.007  trn_regr_loss: 0.151  trn_objectness_loss: 0.003  trn_rpn_box_reg_loss: 0.007  val_regr_loss: 0.184  val_loc_loss: 0.108  trn_loss: 0.239  val_loss: 0.307  (683.10s - -546.48s remaining)\n",
            "EPOCH: 6.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.072  val_objectness_loss: 0.008  trn_regr_loss: 0.144  trn_objectness_loss: 0.003  trn_rpn_box_reg_loss: 0.006  val_regr_loss: 0.186  val_loc_loss: 0.112  trn_loss: 0.225  val_loss: 0.314  (794.30s - -661.92s remaining)\n",
            "EPOCH: 7.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.068  val_objectness_loss: 0.008  trn_regr_loss: 0.137  trn_objectness_loss: 0.002  trn_rpn_box_reg_loss: 0.006  val_regr_loss: 0.182  val_loc_loss: 0.109  trn_loss: 0.214  val_loss: 0.307  (905.80s - -776.40s remaining)\n",
            "EPOCH: 8.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.065  val_objectness_loss: 0.008  trn_regr_loss: 0.135  trn_objectness_loss: 0.003  trn_rpn_box_reg_loss: 0.006  val_regr_loss: 0.186  val_loc_loss: 0.107  trn_loss: 0.209  val_loss: 0.308  (1017.36s - -890.19s remaining)\n",
            "EPOCH: 9.000  val_rpn_box_reg_loss: 0.007  trn_loc_loss: 0.061  val_objectness_loss: 0.008  trn_regr_loss: 0.126  trn_objectness_loss: 0.002  trn_rpn_box_reg_loss: 0.006  val_regr_loss: 0.184  val_loc_loss: 0.110  trn_loss: 0.194  val_loss: 0.310  (1129.52s - -1004.02s remaining)\n",
            "EPOCH: 10.000  val_rpn_box_reg_loss: 0.008  trn_loc_loss: 0.061  val_objectness_loss: 0.010  trn_regr_loss: 0.126  trn_objectness_loss: 0.002  trn_rpn_box_reg_loss: 0.005  val_regr_loss: 0.179  val_loc_loss: 0.111  trn_loss: 0.195  val_loss: 0.308  (1241.54s - -1117.39s remaining)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper methods for Predection"
      ],
      "metadata": {
        "id": "DxpLJe7vcKjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_batch(batch, model, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    X, _ = unbatch(batch, device = device)\n",
        "    predictions = model(X)\n",
        " #   predictions= decode_prediction(prediction, score_threshold=0.9, nms_iou_threshold=0.3)\n",
        "    predictions = [decode_prediction(pred, score_threshold=0.8, nms_iou_threshold=0.3) for pred in predictions]\n",
        "    return [x.cpu() for x in X], predictions\n",
        "def predict(model, data_loader, device = \"cpu\"):\n",
        "\n",
        "    images = []\n",
        "    predictions = []\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        X, p = predict_batch(batch, model, device)\n",
        "        images = images + X\n",
        "        predictions = predictions + p\n",
        "    return images, predictions"
      ],
      "metadata": {
        "id": "nStydINGty8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_prediction(prediction,\n",
        "                      score_threshold ,\n",
        "                      nms_iou_threshold ):\n",
        "    \"\"\"\n",
        "    Inputs\n",
        "        prediction: dict\n",
        "        score_threshold: float\n",
        "        nms_iou_threshold: float\n",
        "    Returns\n",
        "        prediction: tuple\n",
        "    \"\"\"\n",
        "    boxes = prediction[\"boxes\"]\n",
        "    scores = prediction[\"scores\"]\n",
        "    labels = prediction[\"labels\"]\n",
        "    # Remove any low-score predictions.\n",
        "    if score_threshold is not None:\n",
        "        want = scores > score_threshold\n",
        "        boxes = boxes[want]\n",
        "        scores = scores[want]\n",
        "        labels = labels[want]\n",
        "    # Remove any overlapping bounding boxes using NMS.\n",
        "    if nms_iou_threshold is not None:\n",
        "        want = torchvision.ops.nms(boxes = boxes, scores = scores,\n",
        "                                iou_threshold = nms_iou_threshold)\n",
        "        boxes = boxes[want]\n",
        "        scores = scores[want]\n",
        "        labels = labels[want]\n",
        "    return {\n",
        "        \"boxes\": boxes.cpu().numpy(),\n",
        "        \"labels\": labels.cpu().numpy(),\n",
        "        \"scores\": scores.cpu().numpy()\n",
        "    }"
      ],
      "metadata": {
        "id": "vSCm_Ceity5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countpred (predictions):\n",
        "  P_counts=[]\n",
        "  for i in range(len(predictions)):\n",
        "      P_counts.append(np.array(predictions[i]['labels']))#.cpu()\n",
        "  # Determine the total number of classes\n",
        "  num_classes = 3\n",
        "  # Initialize an array to store the sum of each class in each image\n",
        "  P_sums = np.zeros((len(P_counts), num_classes), dtype=int)\n",
        "\n",
        "  # Calculate the sum of each class in each image\n",
        "  for i, counts in enumerate(P_counts):\n",
        "      unique_classes, class_counts = np.unique(counts, return_counts=True)\n",
        "      P_sums[i, unique_classes-1] = class_counts\n",
        "  return P_sums"
      ],
      "metadata": {
        "id": "32LEqQeqzq7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def counttrue (dataset):\n",
        "    T_counts_list=[]\n",
        "    for idx in range(len(dataset)):\n",
        "        # Get the image and its target annotations\n",
        "        _, target = dataset[idx]\n",
        "\n",
        "        # Extract labels for the current image\n",
        "        labels = target['labels']\n",
        "        #print(labels)\n",
        "\n",
        "        # Count occurrences of each label\n",
        "        mask_on_count = (labels == 1).sum().item()  #  1 represents mask on correctly\n",
        "        no_mask_count = (labels == 2).sum().item()  #  2 represents no mask\n",
        "        mask_incorrect_count = (labels == 3).sum().item()  #  3 represents mask worn incorrectly\n",
        "\n",
        "        # Append the counts for the current image to the list\n",
        "        T_counts_list.append([mask_on_count, no_mask_count, mask_incorrect_count])\n",
        "\n",
        "    counts_array = np.array(T_counts_list, dtype=np.int64)\n",
        "\n",
        "    return counts_array"
      ],
      "metadata": {
        "id": "xzfs2cod3b4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final method"
      ],
      "metadata": {
        "id": "1mvibH1uSlFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_masks(dataset):\n",
        "\n",
        "    dataset_dl = torch.utils.data.DataLoader(dataset,batch_size = 2,shuffle = False, collate_fn = collate_fn)\n",
        "    # Initialize counts\n",
        "\n",
        "    MAPE=0\n",
        "\n",
        "    images, predictions = predict(model, dataset_dl, device)\n",
        "    P_list =countpred (predictions)\n",
        "    T_list = counttrue (dataset)\n",
        "\n",
        "    nims,ncls=T_list.shape[0],T_list.shape[1]\n",
        "    # mape for each image\n",
        "    mape=np.zeros(nims)\n",
        "    #mape for each class in image\n",
        "    class_mape=np.zeros(ncls)\n",
        "\n",
        "    for i in range(nims):\n",
        "      for t in range(ncls):\n",
        "        class_mape[t]= np.abs((T_list[i][t] - P_list[i][t] )/np.max([T_list[i][t],1]))*100\n",
        "      mape[i]=np.mean(class_mape)\n",
        "    MAPE=np.mean(mape)\n",
        "\n",
        "    print(f'''\n",
        "            MAPE : { round(MAPE, 2) } %\n",
        "            ''')\n",
        "\n",
        "    return T_list , MAPE"
      ],
      "metadata": {
        "id": "qN2FFs46D8vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MaskedFaceDataset('/content/drive/MyDrive/Colab Notebooks/MaskedFace/val', transforms = get_transform(train=False))\n",
        "T_list , MAPE =count_masks(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ3SKrk7-NE7",
        "outputId": "a06ef832-c40d-417a-83ed-7979f2f381ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "            MAPE : 12.8 % \n",
            "            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAPE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBbK8pM2Qlko",
        "outputId": "02f2b15c-02c7-4383-91ad-09dbc2f7bd99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.798514146994336"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t14XXu3L8hN-",
        "outputId": "f2aad1a2-1d1f-42ac-ca04-360be5b5791a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15,  0,  1],\n",
              "       [ 3,  3,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 2,  0,  0],\n",
              "       [ 9,  0,  0],\n",
              "       [ 9,  0,  0],\n",
              "       [ 2,  0,  0],\n",
              "       [13,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 2,  2,  0],\n",
              "       [ 4,  1,  1],\n",
              "       [ 0,  1,  0],\n",
              "       [ 4,  0,  0],\n",
              "       [ 2,  0,  0],\n",
              "       [ 0,  2,  2],\n",
              "       [ 4,  4,  0],\n",
              "       [17,  1,  1],\n",
              "       [ 1,  0,  0],\n",
              "       [ 3,  3,  0],\n",
              "       [ 6,  0,  0],\n",
              "       [ 7,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  1,  0],\n",
              "       [10,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [10,  0,  0],\n",
              "       [ 2, 12,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [12,  0,  0],\n",
              "       [ 0,  1,  0],\n",
              "       [13,  0,  0],\n",
              "       [53,  0,  0],\n",
              "       [ 2,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [15,  5,  1],\n",
              "       [ 2,  0,  0],\n",
              "       [ 2,  0,  0],\n",
              "       [19, 16,  0],\n",
              "       [ 3,  7,  4],\n",
              "       [ 1,  1,  1],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 9,  0,  0],\n",
              "       [ 0,  1,  0],\n",
              "       [ 0,  0,  1],\n",
              "       [11,  2,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 2,  1,  1],\n",
              "       [20,  0,  1],\n",
              "       [ 7,  1,  0],\n",
              "       [ 3,  1,  0],\n",
              "       [ 4,  0,  0],\n",
              "       [ 9,  0,  0],\n",
              "       [ 7,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 0,  1,  0],\n",
              "       [ 5,  0,  0],\n",
              "       [ 4,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 0,  0,  1],\n",
              "       [ 6,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 8,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 2,  0,  0],\n",
              "       [ 2,  0,  0],\n",
              "       [ 3,  0,  0],\n",
              "       [ 4,  1,  0],\n",
              "       [ 6,  0,  0],\n",
              "       [ 0,  1,  0],\n",
              "       [19,  1,  0],\n",
              "       [ 1,  6,  0],\n",
              "       [ 8,  1,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [13,  0,  0],\n",
              "       [ 4,  0,  0],\n",
              "       [ 1,  0,  0],\n",
              "       [ 9,  3,  0],\n",
              "       [ 1,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving model"
      ],
      "metadata": {
        "id": "B7PBdHTz-_FG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the array to an .npy file\n",
        "np.save('MAPE.npy', MAPE)\n",
        "np.save('Truelist.npy', T_list)"
      ],
      "metadata": {
        "id": "edbREcyt2oUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'Q7_model.pth')"
      ],
      "metadata": {
        "id": "X-nTCBnt-qWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "id": "sFAliWCa-5Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w35Xnsoi-7gU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}